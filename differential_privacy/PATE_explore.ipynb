{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential privacy section project\n",
    "\n",
    "This work is part of [Udacity](http://udacity.com) 'Secure and Private AI scholarship challenge nanodegree Program'. \n",
    "\n",
    "I study there how to protect customer personal data and still be able to train good ML models.\n",
    "Here is my final project in section \"Differential privacy.\"\n",
    "In this project, I'm going to train a Differentially private model using PATE method on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PATE method?\n",
    "Private Aggregation of Teacher Ensembles or PATE.\n",
    "In this approach, the model is not trained on sensitive data directly. \n",
    "Instead, multiple models trained on different subsets of users. These models called **teachers**.\n",
    "Then, the **student** model trained on the noisy output of **teachers** models.\n",
    "\n",
    "As result **student** model doesn't have direct access to data available to **teachers**, but still can make accurate predictions if **teachers** models generalize data very well.\n",
    "\n",
    "### Points of complexity\n",
    "- It is not always possible to identify people in datasets. \n",
    "       One person could write multiple handwritten digits in one set.\n",
    "- Naural models rarely have the same state, even if it was trained on the same dataset.\n",
    "\n",
    "### Task description\n",
    "\n",
    "Let's imagine that we try to predict some labels, but our dataset unlabeled. However, we know that multiple different organizations have labeled data, but they are not going to share raw datasets with us. So we ask these organizations to train models on their side and provide us predictions for our dataset. Then we could label dataset available to us and train our model. Finally, we have to make sure that predictions that were provided for us are protected and there is no privacy leakage.\n",
    "\n",
    "For this task, MNIST dataset should be separated into **teacher**(private) and **student**(public) parts. Each **teacher** dataset will contains both data(images) and targets(labels). **Student** dataset only contains 'data'. After that, ~40 **teachers** models trained on their datasets. Then, each trained model predict targets using public data. Each sample should have ~40 estimations, and the most popular answer selected as a label. The label should not depend on a prediction from a particular teacher, so each label should have some amount of noise. Finally, **student** model trained on a labeled dataset. True labels are removed from **student** datasets artificially in our case. As a result, we could make a decent estimation of prediction quality.\n",
    "\n",
    "The goal is to answer the following questions:\n",
    "\n",
    "- Id it keep *teachers* data private?\n",
    "\n",
    "- Is it possible to train **student** without a massive loss in prediction quality?\n",
    "\n",
    "- What is a necessary amount of noise to add to keep data private?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare datasets for **teachers** and **student** \n",
    "\n",
    "**PyTorch** is the main framework to build and train models in this project. \n",
    "That means that input dataset should be tensors.\n",
    "So, let's download MNIST dataset and make the necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                              ])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = data_utils.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = data_utils.DataLoader(mnist_testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST train size 60000\n",
      "MNIST test size 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"MNIST train size {len(mnist_trainset)}\")\n",
    "print(f\"MNIST test size {len(mnist_testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use **PyTorch** build-in function `random_split` to split original dataset. `teachers_and_student_datasets` is a convenience function to divide dataset to equally between student and teachers. It helps us to avoid some complexity at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teachers_and_student_datasets(original_dataset, subset_size, parts, verbose=True):\n",
    "    '''divide original dataset to equal subsets'''\n",
    "    total = len(original_dataset)\n",
    "    lengths = [subset_size] * parts + [total - subset_size * parts]\n",
    "    if verbose:\n",
    "        print(f\"Subsets lengths {lengths}\")\n",
    "    datasets = data_utils.dataset.random_split(original_dataset, lengths)\n",
    "    return (datasets[:parts - 1], datasets[parts-1])\n",
    "    \n",
    "def to_dataloader(dataset):\n",
    "    return data_utils.DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train model each **teacher** and **student** should have train and test dataloaders. Also, **teacher** name is useful to track training progress. So, it is not a bad idea to group this data to one named tuple and store together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "DataInfo = namedtuple('DataInfo', 'name train test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was mentioned earlier, **student** dataset only includes images, not labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets):\n",
    "    teachers_train_dl = [to_dataloader(d) for d in teachers_trainsets]\n",
    "    teachers_test_dl = [to_dataloader(d) for d in teachers_testsets] \n",
    "\n",
    "    teachers_data =[DataInfo(a[0], a[1], a[2]) for a in zip(teachers_names, teachers_train_dl, teachers_test_dl)]\n",
    "    return teachers_data\n",
    "\n",
    "def init_students_data(student_name, student_trainset, student_testset):\n",
    "    # train set doesn't have lables\n",
    "    student_train_dl = to_dataloader([img for img, lbl in student_trainset])\n",
    "    student_test_dl = to_dataloader(student_testset)\n",
    "\n",
    "    student_data = DataInfo(student_name, student_train_dl, student_test_dl)\n",
    "    return student_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our infrastructure on toy dataset first. In toy datasets, there are only 6 **teachers** with a set of 100 images each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers_names = ['El', 'Max', 'Dustin', 'Will', 'Lukas', 'Mike']\n",
    "student_name = \"Demogorgon\"\n",
    "subset_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 59300]\n",
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 9300]\n"
     ]
    }
   ],
   "source": [
    "N = len(teachers_names)\n",
    "teachers_trainsets, student_trainset = teachers_and_student_datasets(mnist_trainset, subset_size, N + 1)\n",
    "teachers_testsets, student_testset = teachers_and_student_datasets(mnist_testset, subset_size, N + 1)\n",
    "\n",
    "teachers_data = init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets)\n",
    "student_data = init_students_data(student_name, student_trainset, student_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataInfo(name='Demogorgon', train=<torch.utils.data.dataloader.DataLoader object at 0x1061e5208>, test=<torch.utils.data.dataloader.DataLoader object at 0x1061e54e0>)\n",
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{student_data}\")\n",
    "print(f\"{next(iter(student_data.train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "DataInfo(name='Dustin', train=<torch.utils.data.dataloader.DataLoader object at 0x11dc69898>, test=<torch.utils.data.dataloader.DataLoader object at 0x11dc69c88>)\n",
      "[tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]), tensor([8, 3, 2, 4, 5, 3, 1, 1, 5, 7, 1, 3, 9, 9, 7, 2, 5, 9, 1, 0, 8, 1, 7, 2,\n",
      "        7, 0, 9, 2, 9, 5, 3, 1, 6, 6, 1, 4, 1, 4, 8, 0, 7, 7, 3, 9, 2, 0, 6, 4,\n",
      "        1, 4, 7, 9, 3, 2, 1, 5, 3, 6, 2, 7, 5, 3, 3, 3])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(teachers_data)}\")\n",
    "print(f\"{teachers_data[2]}\")\n",
    "print(f\"{next(iter(teachers_data[2].train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Select a model\n",
    "Building a perfect predictor is not the primary goal in this project, and the dataset is very simple. So, an unsophisticated, fully connected neural network should be a good fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleModel object is very general, so neither PATE approach or MNIST dataset is mentioned in an object definition. This generalization is an advantage so that it could be easily replaced and changed or served for a different purpose. \n",
    "In this project, **student** and **teachers**  use the same type of model, but this is not required. In real-world, a trained model should be eighter shared as a file or, even better, has a dedicated prediction service with proper logs and authorization. However, I'll skip this part for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.utils.data as ds\n",
    "import functools\n",
    "\n",
    "class SimpleModel():\n",
    "    '''\n",
    "    This is a simple 3-layers fully connected neural network for supervised learning\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name is used to identify model and to track training progress\n",
    "    trainloader : torch.utils.data.dataloader.DataLoader \n",
    "        Dataloader with train dataset\n",
    "    testloader : torch.utils.data.dataloader.DataLoader\n",
    "        Dataloader with test dataset\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str \n",
    "        This is where we store model name\n",
    "    model :\n",
    "        This is where we store model\n",
    "    train_epochs : int\n",
    "        Amount of epochs used to train model (the default is 5)\n",
    "    accuracy : float\n",
    "        Store model's accuracy from the last validation round\n",
    "    is_trained : bool\n",
    "        Flag that shows if model training already finished\n",
    "    '''\n",
    "    \n",
    "    INPUT_SIZE = 784\n",
    "    OUTPUT_SIZE = 10\n",
    "    \n",
    "    def __init__(self, name, trainloader, testloader):\n",
    "        super().__init__()\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.name = name\n",
    "        self.train_epochs = 5\n",
    "        self._accuracy = 0\n",
    "        self._model = None\n",
    "        self._trained = False\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        if self._model is None:\n",
    "            self._model = nn.Sequential(nn.Linear(self.INPUT_SIZE, 256),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(256, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(64, self.OUTPUT_SIZE),\n",
    "                                        nn.LogSoftmax(dim=1))\n",
    "        return self._model\n",
    "                                        \n",
    "    @property\n",
    "    def is_trained(self):\n",
    "        return self._trained\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return self._accuracy\n",
    "    \n",
    "    def predict_labels(self, images):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            images = self.__flatten_input__(images)\n",
    "            pred = torch.exp(self.model(images))\n",
    "            # get one with highest probablity\n",
    "            top_prob, top_class = pred.topk(1, dim=1)\n",
    "            self.model.train()\n",
    "        return top_class\n",
    "\n",
    "    def test_model(self, dataloader):\n",
    "        accuracy = 0\n",
    "        for images, labels in dataloader:\n",
    "            pred_labels = self.predict_labels(images)\n",
    "            accuracy += self.__prediction_accuracy__(labels, pred_labels)\n",
    "        return accuracy/len(dataloader)\n",
    "\n",
    "    def train_model(self, verbose=True):\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.005)\n",
    "        self.__opt_print__(f\"{self.name} starts training\", verbose)\n",
    "        for e in range(self.train_epochs):\n",
    "            for images, labels in self.trainloader:\n",
    "                images = self.__flatten_input__(images)\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                self._accuracy = self.test_model(self.testloader)\n",
    "                self.__opt_print__(f\"Improved accuracy: {self._accuracy}\", verbose)\n",
    "        self._trained = True\n",
    "        self.__opt_print__(f\"{self.name} finished training\", verbose)\n",
    "        return self._model\n",
    "    \n",
    "    def __flatten_input__(self, images):\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        assert(images.shape[1] == self.INPUT_SIZE)\n",
    "        return images\n",
    "    \n",
    "    def __prediction_accuracy__(self, true_labels, pred_labels):\n",
    "        '''helps to compare two tensors and return how similar they are in range 0...1'''\n",
    "        equals = pred_labels == true_labels.view(*pred_labels.shape)\n",
    "        return torch.mean(equals.type(torch.FloatTensor))\n",
    "    \n",
    "    def __opt_print__(self, message, verbose=True):\n",
    "        if verbose:\n",
    "            print(message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real live **teachers** would be trained on separate machines in parallel. However, because we just fake private datasets **teachers** are trained sequentially and stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El starts training\n",
      "Improved accuracy: 0.3862847089767456\n",
      "Improved accuracy: 0.6006944179534912\n",
      "Improved accuracy: 0.6380208730697632\n",
      "Improved accuracy: 0.7777777910232544\n",
      "Improved accuracy: 0.8098958730697632\n",
      "El finished training\n",
      "Max starts training\n",
      "Improved accuracy: 0.3255208134651184\n",
      "Improved accuracy: 0.4314236044883728\n",
      "Improved accuracy: 0.5633680820465088\n",
      "Improved accuracy: 0.6475694179534912\n",
      "Improved accuracy: 0.7013888955116272\n",
      "Max finished training\n",
      "Dustin starts training\n",
      "Improved accuracy: 0.2873263955116272\n",
      "Improved accuracy: 0.6336805820465088\n",
      "Improved accuracy: 0.7760416269302368\n",
      "Improved accuracy: 0.7838541269302368\n",
      "Improved accuracy: 0.8133680820465088\n",
      "Dustin finished training\n",
      "Will starts training\n",
      "Improved accuracy: 0.3663194477558136\n",
      "Improved accuracy: 0.6597222089767456\n",
      "Improved accuracy: 0.6302083730697632\n",
      "Improved accuracy: 0.7074652910232544\n",
      "Improved accuracy: 0.7109375\n",
      "Will finished training\n",
      "Lukas starts training\n",
      "Improved accuracy: 0.1397569477558136\n",
      "Improved accuracy: 0.4210069477558136\n",
      "Improved accuracy: 0.5842013955116272\n",
      "Improved accuracy: 0.6762152910232544\n",
      "Improved accuracy: 0.6258680820465088\n",
      "Lukas finished training\n",
      "Mike starts training\n",
      "Improved accuracy: 0.1163194477558136\n",
      "Improved accuracy: 0.5260416269302368\n",
      "Improved accuracy: 0.5416666269302368\n",
      "Improved accuracy: 0.6736111044883728\n",
      "Improved accuracy: 0.6380208730697632\n",
      "Mike finished training\n"
     ]
    }
   ],
   "source": [
    "teachers = []\n",
    "for (name, trainloader, testloader) in teachers_data:\n",
    "    t = SimpleModel(name, trainloader, testloader)\n",
    "    t.train_model()\n",
    "    teachers.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks fine. Datasets are small and are not very good predictors as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Public dataset labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to get a prediction from each teacher for every sample. This is function `predict_labels_per_teacher` is for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_per_teacher(images, teachers):\n",
    "    batch_size = images.shape[0]\n",
    "    preds = np.empty([0, batch_size], dtype=int)\n",
    "    for t in teachers:\n",
    "        p = t.predict_labels(images).view(1, batch_size).numpy()\n",
    "        preds = np.append(preds, p, axis=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, for each sample, we should identify the most popular prediction using `bincount` and `argmax` functions and add noise to a prediction.\n",
    "\n",
    "In function `select_best_labels` we use differential privacy to protect customers data.\n",
    "If every teacher generalizes their models very well, then all teachers should agree on every label. In this case, we don't need to add noise to protect personal data: any teacher could be removed, but predicted labels stay the same. \n",
    "On the other hand, if different teacher predicts different labels for the same sample than more noise required to protect personal data. \n",
    "\n",
    "**Why should we add noise?** Imagine that **teachers** are hospitals and our model try to predict some sensitive information, for example, some disease. Also, let's imagine that all hospitals don't agree much in their predictions. In this case, if at least one hospital is removed from the process, predicted labels would change. In this case, we could deduct removed hospital bias and make an estimation of what diseases its customers most probably had. If the disease is rare than given information could be even linked to a real person. (although it requires access to some other private datasets or information).\n",
    "\n",
    "**How much noise to add?** \n",
    "In this project, we generated noise using Laplace distribution with a peak around 0 and beta defined as: \n",
    "\n",
    "`beta = sensitivity / epsilon`. \n",
    "\n",
    "Sensitivity shows how much information we are leaking with one model prediction request. Epsilon identifies the maximum acceptable privacy leakage. \n",
    "\n",
    "Sensitivity value: \n",
    "In the worst-case, each teacher associated with exactly one person. So, at maximum, one person can change a label prediction by 1. In this case, the sensitivity of model prediction per each sample per teacher = 1. \n",
    "Epsilon value:\n",
    "In general, we want to minimize epsilon. However, if it is too small, the beta should be huge, that means an enormous amount of noise in predictions. Noisy predictions lead to the poor performance of **student** model. So, we start with a value 0.25 and then try to minimize it, without a significant loss in prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_labels(preds, use_noise=True, eps=0.25, num_labels=10):\n",
    "    num_samples = preds.shape[1]\n",
    "    labels = np.empty([0], dtype=int)\n",
    "    for i in range(num_samples):\n",
    "        counts = np.bincount(preds[:, i], minlength=num_labels)\n",
    "        if use_noise:\n",
    "            sensitivity = 1\n",
    "            beta = sensitivity / eps\n",
    "            noise = np.random.laplace(0, beta, len(counts)).astype(int)\n",
    "            counts += noise\n",
    "        labels = np.append(labels, np.argmax(counts))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function ```perform_analysis```  helps to identify optimal epsilon value.\n",
    "Framework **PySyft** contains handy function `pate.perform_analysis` for this kind of tasks. It helps to identify how much teachers agree with each other to identify optimal epsilon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 09:56:31.861488 4401046976 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/miniconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0727 09:56:31.877311 4401046976 deprecation_wrapper.py:119] From /miniconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "def perform_analysis(imageset, teachers, use_noise=True, eps=0.25, num_labels=10):\n",
    "    all_preds = np.empty([len(teachers), 0], dtype=int)\n",
    "    all_labels = np.empty([0], dtype=int)\n",
    "    for images in imageset:\n",
    "        preds = predict_labels_per_teacher(images, teachers)\n",
    "        labels = select_best_labels(preds, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "        all_preds = np.append(all_preds, preds, axis=1)\n",
    "        all_labels = np.append(all_labels, labels)\n",
    "    return pate.perform_analysis(teacher_preds=all_preds, indices=all_labels, noise_eps=eps, delta=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how new function works on a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 36.51292546497023\n",
      "Data Dependent Epsilon: 36.51292546497023\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = perform_analysis(student_data.train, teachers, use_noise=True, eps=0.25, num_labels=10)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Independent and Dependent Epsilon are very close. There could be at least two reasons why this happened. Either our teachers predict labels almost randomly, or there is too much noise added. In this case, it is both. Indeed Laplace distribution with `beta = 1 / 0.25` generates noise that alters predictions completely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's create a function ```label_dataset ``` that uses all described functions to create a labeled dataset for **student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dataset(imageset, teachers, use_noise=True, eps=0.25, num_labels=10):\n",
    "    ''' Create labeled dataset from images and teachers that helps to predict it '''\n",
    "    datasets = []\n",
    "    for images in imageset:\n",
    "        preds = predict_labels_per_teacher(images, teachers)\n",
    "        labels = select_best_labels(preds, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "        datasets.append(data_utils.TensorDataset(images, torch.tensor(labels)))\n",
    "    return data_utils.ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Monitor noise impact on prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all three parts and make 'main' function that runs it all together.\n",
    "\n",
    "WARN: It looks like topk function break tie randomly. So if 2 labels have very similar prediction probability accuracy could be slightly different (~0.002). You can see this if you re-rerun `run` function. eval() and train() functions works correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.25, num_labels=SimpleModel.OUTPUT_SIZE):\n",
    "    # generate data from MNIST\n",
    "    N = len(teachers_names)\n",
    "    teachers_trainsets, student_trainset = teachers_and_student_datasets(mnist_trainset, train_subset_size, N + 1, verbose=False)\n",
    "    teachers_testsets, student_testset = teachers_and_student_datasets(mnist_testset, test_subset_size, N + 1, verbose=False)\n",
    "\n",
    "    teachers_data = init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets)\n",
    "    student_data = init_students_data(student_name, student_trainset, student_testset)\n",
    "    \n",
    "    # train teachers models\n",
    "    teachers = []\n",
    "    t_accuracies = []\n",
    "    for (name, trainloader, testloader) in teachers_data:\n",
    "        t = SimpleModel(name, trainloader, testloader)\n",
    "        t.train_model(verbose=False)\n",
    "        t_accuracies.append(t.accuracy)\n",
    "        teachers.append(t)\n",
    "    print(f\"Teachers final accuracies range {min(t_accuracies)} .. {max(t_accuracies)}\")\n",
    "    \n",
    "    # evaluate Epsilon\n",
    "    data_dep_eps, data_ind_eps = perform_analysis(student_data.train, teachers, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "    print(\"Data Independent Epsilon: \\t\", data_ind_eps)\n",
    "    print(\"Data Dependent Epsilon: \\t\", data_dep_eps)   \n",
    "\n",
    "    # create labeled dataset\n",
    "    result_dataset = label_dataset(student_data.train, teachers, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "    result_dataloader = data_utils.DataLoader(result_dataset, batch_size=64, shuffle=True)\n",
    "    result_model = SimpleModel(student_data.name, result_dataloader, student_data.test)\n",
    "    result_model.train_model(verbose=False)\n",
    "    print(f\"Student model accuracy {result_model.accuracy}\")\n",
    "    \n",
    "    # get new model accuracy estimation using whole test dataset\n",
    "    accuracy = result_model.test_model(mnist_testloader)\n",
    "    print(f\"Student model accuracy (big dataset) {accuracy}\")\n",
    "    return (accuracy, data_dep_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main dataset include 49 **teachers** and 1 **student**. The more teachers we use, then easier to find correct labels, easier to generalize data, and easier to protect personal data of every teacher. So, we can start with a smaller epsilon. \n",
    "\n",
    "Let's imagine that our teachers are real offices, that has scans of handwritten documents that contain personal information of its employees. Let's check how good can be **student** model if we try to protect **teachers** personal data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 49\n",
    "teachers_names = [f'office_{i}' for i in range(49)]\n",
    "student_name = \"Reseach office\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, lets use all available dataset and train teachers with decent dataset size.\n",
    "\n",
    "But before we start tuning epsilon, lets find out how good could be student model without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.80078125 .. 0.94921875\n",
      "Data Independent Epsilon: \t 203.51292546497027\n",
      "Data Dependent Epsilon: \t 59.35584801489669\n",
      "Student model accuracy 0.875\n",
      "Student model accuracy (big dataset) 0.8814689517021179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8815), 59.35584801489669)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expample 0\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=False, eps=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student** prediction accuracy is quite hight: **0.8814**. However, if we do not add any noise, we still leak some amount of personal information. We do not have an estimation on how much. Data Dependent Epsilon and Data Independent Epsilon, not relevant right now, because we do not use any epsilon. Data Dependent Epsilon gives us a clue that teachers most ofter agree in their estimations, but not always. Let's try to add some noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.8125 .. 0.94140625\n",
      "Data Independent Epsilon: \t 203.51292546497027\n",
      "Data Dependent Epsilon: \t 6.102855927835513\n",
      "Student model accuracy 0.8984375\n",
      "Student model accuracy (big dataset) 0.8893312215805054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8893), 6.102855927835513)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expample 1\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New **student** prediction accuracy: **0.8893**.\n",
    "**Student** model accuracy didn't drop down because of additional noise.\n",
    "\n",
    "Now the difference between Data Independent Epsilon and Data Dependent Epsilon makes more sense. Our epsilon is small, and we do not leak much personal information. The maximum amount of information that we could leak in this setup: **203.51292546497027**\n",
    "\n",
    "Let's try to even smaller epsilon: **0.07**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.8046875 .. 0.9609375\n",
      "Data Independent Epsilon: \t 35.03292546497023\n",
      "Data Dependent Epsilon: \t 35.03292546497056\n",
      "Student model accuracy 0.84375\n",
      "Student model accuracy (big dataset) 0.8389729261398315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8390), 35.03292546497056)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expample 2\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New **student** prediction accuracy: **0.8390**.\n",
    "**Student** model accuracy dropped down.\n",
    "\n",
    "As we can see here, Data Independent Epsilon and Data Dependent Epsilon are very close to each other again. So, there is too much noise added. Data Dependent Epsilon shows that teachers do not agree very much, but  Data Independent Epsilon definitelly much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something between **0.07** and **0.2**. So, we add less noise than in Example 2 but more than in Example 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.78515625 .. 0.94921875\n",
      "Data Independent Epsilon: \t 105.59292546497024\n",
      "Data Dependent Epsilon: \t 26.05819516700925\n",
      "Student model accuracy 0.88671875\n",
      "Student model accuracy (big dataset) 0.8849522471427917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8850), 26.05819516700925)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expample 3\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New **student** prediction accuracy: **0.8850**.\n",
    "This prediction is much better than in Example 2 and not much worse than in Example 1 and Example 0.\n",
    "\n",
    "The maximum amount of information that we could leak in the worst case with this setup: **105.59292546497024**. Data Dependent Epsilon is low, so teachers still mostly agreed with generated labels. So, this is the best setup we had so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test a **student** model with bigger epsilon to make sure that we are moving in the right direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.796875 .. 0.9375\n",
      "Data Independent Epsilon: \t 311.51292546497024\n",
      "Data Dependent Epsilon: \t 5.593521812289911\n",
      "Student model accuracy 0.8828125\n",
      "Student model accuracy (big dataset) 0.8932125568389893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8932), 5.593521812289911)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expample 4\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like epsilon=0.25 has lowest possible Data Dependent Epsilon estimation. This is not completely true. Because of random nature of noise, Data Dependent Epsilon could increase sighnificantelly if we re-run this cell. So, there is not enough noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary 1** \n",
    "\n",
    "We had trained 49 teachers, each of them predict accuracy very well: **0.77 .. 0.96**\n",
    "Epsilon  **0.14** is the best choice so far. We minimize total epsilon but still keep decent accuracy. We could tune model, even more, using values **(0.07 .. 0.014)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is not always possible to increase the amount of teacher(thought it might be useful) or force each of them to increase their dataset. \n",
    "Let's have a look at an example where our teachers have very limited datasets. \n",
    "We should add more noise at the very beginning to protect data even more. So we start with epsilon 0.25\n",
    "\n",
    "Let's start training without noise at first to estimate baseline model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.5989583730697632 .. 0.8168402910232544\n",
      "Data Independent Epsilon: \t 36.51292546497023\n",
      "Data Dependent Epsilon: \t 10.501805637539771\n",
      "Student model accuracy 0.6779513955116272\n",
      "Student model accuracy (big dataset) 0.6691879034042358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6692), 10.501805637539771)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset_size = 100\n",
    "test_subset_size = 100\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=False, eps=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.5095486044883728 .. 0.8489583730697632\n",
      "Data Independent Epsilon: \t 36.51292546497023\n",
      "Data Dependent Epsilon: \t 7.858616426758116\n",
      "Student model accuracy 0.6354166269302368\n",
      "Student model accuracy (big dataset) 0.6860071420669556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6860), 7.858616426758116)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset_size = 100\n",
    "test_subset_size = 100\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary2**  Noise do not degrade student model accuracy. It is the opposite. It because many teachers suffer from overfitting because of small datasets. In this case, noise gives a chance to second popular answer, that could be a better prediction.\n",
    "\n",
    "It is worse to mention that sensitivity = 1 is a bit too much. When we remove one teacher, most probably we do not immediately remove one person and leak his/her data. If every teacher contains information about hundreds of people, sensitivity could be smaller. As a result, we could use a smaller epsilon. However, this requires some additional analytical work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "In this work was [PySyft](https://github.com/OpenMined/PySyft) was used to perform analysis. This library still in its early days. It is too early to use it to protect customer data. However, in a couple of months, this statement should be reviewed and hopefully removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [(0.3, 0.4), (0.1, 0.8), (0.5, 0.5), (0.4, 0.6)]\n",
    "\n",
    "# plt.scatter(data[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
