{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential privacy section project\n",
    "\n",
    "This work is part of [Udacity](http://udacity.com) 'Secure and Private AI scholarship challenge nanodegree Program'. \n",
    "\n",
    "I study there how to protect customer personal data and still be able to train good ML models.\n",
    "Here is my final project in section \"Differential privacy.\"\n",
    "In this project, I'm going to train a Differentially private model using PATE method on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PATE method?\n",
    "Private Aggregation of Teacher Ensembles or PATE.\n",
    "In this approach, the model is not trained on sensitive data directly. \n",
    "Instead, multiple models trained on different subsets of users. These models called **teachers**.\n",
    "Then, the **student** model trained on the noisy output of **teachers** models.\n",
    "\n",
    "As result **student** model doesn't have direct access to data available to **teachers**, but still can make accurate predictions if **teachers** models generalize data very well.\n",
    "\n",
    "### Points of complexity\n",
    "- It is not always possible to identify people in datasets. \n",
    "       One person could write multiple handwritten digits in one set.\n",
    "- Naural models rarely have the same state, even if it was trained on the same dataset.\n",
    "\n",
    "### Task description\n",
    "\n",
    "Let's imagine that we try to predict some labels, but our dataset unlabeled. However, we know that multiple different organizations have labeled data, but they are not going to share raw datasets with us. So we ask these organizations to train models on their side and provide us predictions for our dataset. Then we could label dataset available to us and train our model. Finally, we have to make sure that predictions that were provided for us are protected and there is no privacy leakage.\n",
    "\n",
    "For this task, MNIST dataset should be separated into **teacher**(private) and **student**(public) parts. Each **teacher** dataset will contains both data(images) and targets(labels). **Student** dataset only contains 'data'. After that, ~40 **teachers** models trained on their datasets. Then, each trained model predict targets using public data. Each sample should have ~40 estimations, and the most popular answer selected as a label. The label should not depend on a prediction from a particular teacher, so each label should have some amount of noise. Finally, **student** model trained on a labeled dataset. True labels are removed from **student** datasets artificially in our case. As a result, we could make a decent estimation of prediction quality.\n",
    "\n",
    "The goal is to answer the following questions:\n",
    "\n",
    "- Id it keep *teachers* data private?\n",
    "\n",
    "- Is it possible to train **student** without a massive loss in prediction quality?\n",
    "\n",
    "- What is a necessary amount of noise to add to keep data private?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare datasets for **teachers** and **student** \n",
    "\n",
    "**PyTorch** is the main framework to build and train models in this project. \n",
    "That means that input dataset should be tensors.\n",
    "So, let's download MNIST dataset and make the necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                              ])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = data_utils.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = data_utils.DataLoader(mnist_testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST train size 60000\n",
      "MNIST test size 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"MNIST train size {len(mnist_trainset)}\")\n",
    "print(f\"MNIST test size {len(mnist_testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use **PyTorch** build-in function `random_split` to split original dataset. `teachers_and_student_datasets` is a convenience function to divide dataset to equally between student and teachers. It helps us to avoid some complexity at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teachers_and_student_datasets(original_dataset, subset_size, parts, verbose=True):\n",
    "    '''divide original dataset to equal subsets'''\n",
    "    total = len(original_dataset)\n",
    "    lengths = [subset_size] * parts + [total - subset_size * parts]\n",
    "    if verbose:\n",
    "        print(f\"Subsets lengths {lengths}\")\n",
    "    datasets = data_utils.dataset.random_split(original_dataset, lengths)\n",
    "    return (datasets[:parts - 1], datasets[parts-1])\n",
    "    \n",
    "def to_dataloader(dataset):\n",
    "    return data_utils.DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train model each **teacher** and **student** should have train and test dataloaders. Also, **teacher** name is useful to track training progress. So, it is not a bad idea to group this data to one named tuple and store together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "DataInfo = namedtuple('DataInfo', 'name train test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was mentioned earlier, **student** dataset only includes images, not labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets):\n",
    "    teachers_train_dl = [to_dataloader(d) for d in teachers_trainsets]\n",
    "    teachers_test_dl = [to_dataloader(d) for d in teachers_testsets] \n",
    "\n",
    "    teachers_data =[DataInfo(a[0], a[1], a[2]) for a in zip(teachers_names, teachers_train_dl, teachers_test_dl)]\n",
    "    return teachers_data\n",
    "\n",
    "def init_students_data(student_name, student_trainset, student_testset):\n",
    "    # train set doesn't have lables\n",
    "    student_train_dl = to_dataloader([img for img, lbl in student_trainset])\n",
    "    student_test_dl = to_dataloader(student_testset)\n",
    "\n",
    "    student_data = DataInfo(student_name, student_train_dl, student_test_dl)\n",
    "    return student_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our infrastructure on toy dataset first. In toy datasets, there are only 6 **teachers** with a set of 100 images each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers_names = ['El', 'Max', 'Dustin', 'Will', 'Lukas', 'Mike']\n",
    "student_name = \"Demogorgon\"\n",
    "subset_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 59300]\n",
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 9300]\n"
     ]
    }
   ],
   "source": [
    "N = len(teachers_names)\n",
    "teachers_trainsets, student_trainset = teachers_and_student_datasets(mnist_trainset, subset_size, N + 1)\n",
    "teachers_testsets, student_testset = teachers_and_student_datasets(mnist_testset, subset_size, N + 1)\n",
    "\n",
    "teachers_data = init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets)\n",
    "student_data = init_students_data(student_name, student_trainset, student_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataInfo(name='Demogorgon', train=<torch.utils.data.dataloader.DataLoader object at 0x11f47a668>, test=<torch.utils.data.dataloader.DataLoader object at 0x10f7332b0>)\n",
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{student_data}\")\n",
    "print(f\"{next(iter(student_data.train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "DataInfo(name='Dustin', train=<torch.utils.data.dataloader.DataLoader object at 0x11fdb2240>, test=<torch.utils.data.dataloader.DataLoader object at 0x11fdb2630>)\n",
      "[tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]), tensor([8, 6, 7, 3, 0, 0, 3, 1, 9, 9, 1, 6, 2, 0, 4, 5, 0, 3, 4, 0, 3, 6, 6, 7,\n",
      "        4, 3, 8, 7, 5, 4, 5, 4, 0, 7, 3, 8, 3, 7, 6, 1, 3, 6, 0, 6, 2, 7, 0, 8,\n",
      "        3, 2, 2, 1, 0, 3, 6, 0, 5, 0, 3, 1, 0, 6, 5, 9])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(teachers_data)}\")\n",
    "print(f\"{teachers_data[2]}\")\n",
    "print(f\"{next(iter(teachers_data[2].train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Select a model\n",
    "Building a perfect predictor is not the primary goal in this project, and the dataset is very simple. So, an unsophisticated, fully connected neural network should be a good fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleModel object is very general, so neither PATE approach or MNIST dataset is mentioned in an object definition. This generalization is an advantage so that it could be easily replaced and changed or served for a different purpose. \n",
    "In this project, **student** and **teachers**  use the same type of model, but this is not required. In real-world, a trained model should be eighter shared as a file or, even better, has a dedicated prediction service with proper logs and authorization. However, I'll skip this part for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.utils.data as ds\n",
    "import functools\n",
    "\n",
    "class SimpleModel():\n",
    "    '''\n",
    "    This is a simple 3-layers fully connected neural network for supervised learning\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name is used to identify model and to track training progress\n",
    "    trainloader : torch.utils.data.dataloader.DataLoader \n",
    "        Dataloader with train dataset\n",
    "    testloader : torch.utils.data.dataloader.DataLoader\n",
    "        Dataloader with test dataset\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str \n",
    "        This is where we store model name\n",
    "    model :\n",
    "        This is where we store model\n",
    "    train_epochs : int\n",
    "        Amount of epochs used to train model (the default is 5)\n",
    "    accuracy : float\n",
    "        Store model's accuracy from the last validation round\n",
    "    is_trained : bool\n",
    "        Flag that shows if model training already finished\n",
    "    '''\n",
    "    \n",
    "    INPUT_SIZE = 784\n",
    "    OUTPUT_SIZE = 10\n",
    "    \n",
    "    def __init__(self, name, trainloader, testloader):\n",
    "        super().__init__()\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.name = name\n",
    "        self.train_epochs = 5\n",
    "        self._accuracy = 0\n",
    "        self._model = None\n",
    "        self._trained = False\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        if self._model is None:\n",
    "            self._model = nn.Sequential(nn.Linear(self.INPUT_SIZE, 256),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(256, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(64, self.OUTPUT_SIZE),\n",
    "                                        nn.LogSoftmax(dim=1))\n",
    "        return self._model\n",
    "                                        \n",
    "    @property\n",
    "    def is_trained(self):\n",
    "        return self._trained\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return self._accuracy\n",
    "    \n",
    "    def predict_labels(self, images):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            images = self.__flatten_input__(images)\n",
    "            pred = torch.exp(self.model(images))\n",
    "            # get one with highest probablity\n",
    "            top_prob, top_class = pred.topk(1, dim=1)\n",
    "            self.model.train()\n",
    "        return top_class\n",
    "\n",
    "    def test_model(self, dataloader):\n",
    "        accuracy = 0\n",
    "        for images, labels in dataloader:\n",
    "            pred_labels = self.predict_labels(images)\n",
    "            accuracy += self.__prediction_accuracy__(labels, pred_labels)\n",
    "        return accuracy/len(dataloader)\n",
    "\n",
    "    def train_model(self, verbose=True):\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.005)\n",
    "        self.__opt_print__(f\"{self.name} starts training\", verbose)\n",
    "        for e in range(self.train_epochs):\n",
    "            for images, labels in self.trainloader:\n",
    "                images = self.__flatten_input__(images)\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                self._accuracy = self.test_model(self.testloader)\n",
    "                self.__opt_print__(f\"Improved accuracy: {self._accuracy}\", verbose)\n",
    "        self._trained = True\n",
    "        self.__opt_print__(f\"{self.name} finished training\", verbose)\n",
    "        return self._model\n",
    "    \n",
    "    def __flatten_input__(self, images):\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        assert(images.shape[1] == self.INPUT_SIZE)\n",
    "        return images\n",
    "    \n",
    "    def __prediction_accuracy__(self, true_labels, pred_labels):\n",
    "        '''helps to compare two tensors and return how similar they are in range 0...1'''\n",
    "        equals = pred_labels == true_labels.view(*pred_labels.shape)\n",
    "        return torch.mean(equals.type(torch.FloatTensor))\n",
    "    \n",
    "    def __opt_print__(self, message, verbose=True):\n",
    "        if verbose:\n",
    "            print(message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real live **teachers** would be trained on separate machines in parallel. However, because we just fake private datasets **teachers** are trained sequentially and stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El starts training\n",
      "Improved accuracy: 0.1987847238779068\n",
      "Improved accuracy: 0.4782986044883728\n",
      "Improved accuracy: 0.5546875\n",
      "Improved accuracy: 0.6614583730697632\n",
      "Improved accuracy: 0.7222222089767456\n",
      "El finished training\n",
      "Max starts training\n",
      "Improved accuracy: 0.2109375\n",
      "Improved accuracy: 0.5295138955116272\n",
      "Improved accuracy: 0.7065972089767456\n",
      "Improved accuracy: 0.7248263955116272\n",
      "Improved accuracy: 0.7560763955116272\n",
      "Max finished training\n",
      "Dustin starts training\n",
      "Improved accuracy: 0.3862847089767456\n",
      "Improved accuracy: 0.3958333134651184\n",
      "Improved accuracy: 0.4670138955116272\n",
      "Improved accuracy: 0.5651041269302368\n",
      "Improved accuracy: 0.6597222089767456\n",
      "Dustin finished training\n",
      "Will starts training\n",
      "Improved accuracy: 0.2873263955116272\n",
      "Improved accuracy: 0.3498263955116272\n",
      "Improved accuracy: 0.6241319179534912\n",
      "Improved accuracy: 0.6119791269302368\n",
      "Improved accuracy: 0.7300347089767456\n",
      "Will finished training\n",
      "Lukas starts training\n",
      "Improved accuracy: 0.3654513955116272\n",
      "Improved accuracy: 0.4756944477558136\n",
      "Improved accuracy: 0.6015625\n",
      "Improved accuracy: 0.6215277910232544\n",
      "Improved accuracy: 0.6510416269302368\n",
      "Lukas finished training\n",
      "Mike starts training\n",
      "Improved accuracy: 0.3706597089767456\n",
      "Improved accuracy: 0.5182291269302368\n",
      "Improved accuracy: 0.6145833730697632\n",
      "Improved accuracy: 0.6675347089767456\n",
      "Improved accuracy: 0.6814236044883728\n",
      "Mike finished training\n"
     ]
    }
   ],
   "source": [
    "teachers = []\n",
    "for (name, trainloader, testloader) in teachers_data:\n",
    "    t = SimpleModel(name, trainloader, testloader)\n",
    "    t.train_model()\n",
    "    teachers.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks fine. Datasets are small and are not very good predictors as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Public dataset labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to get a prediction from each teacher for every sample. This is function `predict_labels_per_teacher` is for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_per_teacher(images, teachers):\n",
    "    batch_size = images.shape[0]\n",
    "    preds = np.empty([0, batch_size], dtype=int)\n",
    "    for t in teachers:\n",
    "        p = t.predict_labels(images).view(1, batch_size).numpy()\n",
    "        preds = np.append(preds, p, axis=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, for each sample, we should identify the most popular prediction using `bincount` and `argmax` functions and add noise to a prediction.\n",
    "\n",
    "In function `select_best_labels` we use differential privacy to protect customers data.\n",
    "If every teacher generalizes their models very well, then all teachers should agree on every label. In this case, we don't need to add noise to protect personal data: any teacher could be removed, but predicted labels stay the same. \n",
    "On the other hand, if different teacher predicts different labels for the same sample than more noise required to protect personal data. \n",
    "\n",
    "**Why should we add noise?** Imagine that **teachers** are hospitals and our model try to predict some sensitive information, for example, some disease. Also, let's imagine that all hospitals don't agree much in their predictions. In this case, if at least one hospital is removed from the process, predicted labels would change. In this case, we could deduct removed hospital bias and make an estimation of what diseases its customers most probably had. If the disease is rare than given information could be even linked to a real person. (although it requires access to some other private datasets or information).\n",
    "\n",
    "**How much noise to add?** \n",
    "In this project, we generated noise using Laplace distribution with a peak around 0 and beta defined as: \n",
    "\n",
    "`beta = sensitivity / epsilon`. \n",
    "\n",
    "Sensitivity shows how much information we are leaking with one model prediction request. Epsilon identifies the maximum acceptable privacy leakage. \n",
    "\n",
    "Sensitivity value: \n",
    "In the worst-case, each teacher associated with exactly one person. So, at maximum, one person can change a label prediction by 1. In this case, the sensitivity of model prediction per each sample per teacher = 1. \n",
    "Epsilon value:\n",
    "In general, we want to minimize epsilon. However, if it is too small, the beta should be huge, that means an enormous amount of noise in predictions. Noisy predictions lead to the poor performance of **student** model. So, we start with a value 0.25 and then try to minimize it, without a significant loss in prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_labels(preds, use_noise=True, eps=0.25, num_labels=10):\n",
    "    num_samples = preds.shape[1]\n",
    "    labels = np.empty([0], dtype=int)\n",
    "    for i in range(num_samples):\n",
    "        counts = np.bincount(preds[:, i], minlength=num_labels)\n",
    "        if use_noise:\n",
    "            sensitivity = 1\n",
    "            beta = sensitivity / eps\n",
    "            noise = np.random.laplace(0, beta, len(counts)).astype(int)\n",
    "            counts += noise\n",
    "        labels = np.append(labels, np.argmax(counts))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function ```perform_analysis```  helps to identify optimal epsilon value.\n",
    "Framework **PySyft** contains handy function `pate.perform_analysis` for this kind of tasks. It helps to identify how much teachers agree with each other to identify optimal epsilon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 22:17:57.152981 4557661632 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/miniconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0726 22:17:57.189530 4557661632 deprecation_wrapper.py:119] From /miniconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "def perform_analysis(imageset, teachers, use_noise=True, eps=0.25, num_labels=10):\n",
    "    all_preds = np.empty([len(teachers), 0], dtype=int)\n",
    "    all_labels = np.empty([0], dtype=int)\n",
    "    for images in imageset:\n",
    "        preds = predict_labels_per_teacher(images, teachers)\n",
    "        labels = select_best_labels(preds, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "        all_preds = np.append(all_preds, preds, axis=1)\n",
    "        all_labels = np.append(all_labels, labels)\n",
    "    return pate.perform_analysis(teacher_preds=all_preds, indices=all_labels, noise_eps=eps, delta=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how new function works on a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 36.51292546497023\n",
      "Data Dependent Epsilon: 36.51292546497023\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = perform_analysis(student_data.train, teachers, use_noise=True, eps=0.25, num_labels=10)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Independent and Dependent Epsilon are very close. There could be at least two reasons why this happened. Either our teachers predict labels almost randomly, or there is too much noise added. In this case, it is both. Indeed Laplace distribution with `beta = 1 / 0.25` generates noise that alters predictions completely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's create a function ```label_dataset ``` that uses all described functions to create a labeled dataset for **student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dataset(imageset, teachers, use_noise=True, eps=0.25, num_labels=10):\n",
    "    ''' Create labeled dataset from images and teachers that helps to predict it '''\n",
    "    datasets = []\n",
    "    for images in imageset:\n",
    "        preds = predict_labels_per_teacher(images, teachers)\n",
    "        labels = select_best_labels(preds, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "        datasets.append(data_utils.TensorDataset(images, torch.tensor(labels)))\n",
    "    return data_utils.ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Monitor noise impact on prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all three parts and make 'main' function that runs it all together.\n",
    "\n",
    "WARN: It looks like topk function break tie randomly. So if 2 labels have very similar prediction probability accuracy could be slightly different (~0.002). You can see this if you re-rerun `run` function. eval() and train() functions works correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.25, num_labels=SimpleModel.OUTPUT_SIZE):\n",
    "    # generate data from MNIST\n",
    "    N = len(teachers_names)\n",
    "    teachers_trainsets, student_trainset = teachers_and_student_datasets(mnist_trainset, train_subset_size, N + 1, verbose=False)\n",
    "    teachers_testsets, student_testset = teachers_and_student_datasets(mnist_testset, test_subset_size, N + 1, verbose=False)\n",
    "\n",
    "    teachers_data = init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets)\n",
    "    student_data = init_students_data(student_name, student_trainset, student_testset)\n",
    "    \n",
    "    # train teachers models\n",
    "    teachers = []\n",
    "    t_accuracies = []\n",
    "    for (name, trainloader, testloader) in teachers_data:\n",
    "        t = SimpleModel(name, trainloader, testloader)\n",
    "        t.train_model(verbose=False)\n",
    "        t_accuracies.append(t.accuracy)\n",
    "        teachers.append(t)\n",
    "    print(f\"Models final accuracies range {min(t_accuracies)} .. {max(t_accuracies)}\")\n",
    "    \n",
    "    # evaluate Epsilon\n",
    "    data_dep_eps, data_ind_eps = perform_analysis(student_data.train, teachers, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "    print(\"Data Independent Epsilon: \\t\", data_ind_eps)\n",
    "    print(\"Data Dependent Epsilon: \\t\", data_dep_eps)   \n",
    "\n",
    "    # create labeled dataset\n",
    "    result_dataset = label_dataset(student_data.train, teachers, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "    result_dataloader = data_utils.DataLoader(result_dataset, batch_size=64, shuffle=True)\n",
    "    result_model = SimpleModel(student_data.name, result_dataloader, student_data.test)\n",
    "    result_model.train_model(verbose=False)\n",
    "    print(f\"Student model accuracy {result_model.accuracy}\")\n",
    "    \n",
    "    # get new model accuracy estimation using whole test dataset\n",
    "    accuracy = result_model.test_model(mnist_testloader)\n",
    "    print(f\"Student model accuracy (big dataset) {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main dataset will include 49 **teachers** and 1 **student**. The more teachers we use, then easier to find correct labels, easier to generalize data, and easier to protect personal data of every teacher. So, we can start with a smaller epsilon. \n",
    "\n",
    "Lets imagine that our teachers are real fysical offes, that has scans of hand written documents that contains pesonal information of its employees. Lets check how good we can be **student** model if we try to protect **teachers** personal data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 49\n",
    "teachers_names = [f'office_{i}' for i in range(49)]\n",
    "student_name = \"Reseach office\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, lets use all available dataset and train teachers with decent dataset size.\n",
    "\n",
    "But before we start tuning epsilon, lets find out how good could be student model without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.640625 .. 0.9609375\n",
      "Data Independent Epsilon: \t 203.51292546497027\n",
      "Data Dependent Epsilon: \t 51.50558452352363\n",
      "Student model accuracy 0.9140625\n",
      "Student model accuracy (big dataset) 0.8737062215805054\n"
     ]
    }
   ],
   "source": [
    "# Expample 0\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=False, eps=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Data Dependent Epsilon are not equal to Data Independent Epsilon, because teachers mostly agree with each other on their predictions. Lets try to add some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.79296875 .. 0.96484375\n",
      "Data Independent Epsilon: \t 203.51292546497027\n",
      "Data Dependent Epsilon: \t 32.9541128670341\n",
      "Student model accuracy 0.91015625\n",
      "Student model accuracy (big dataset) 0.890625\n"
     ]
    }
   ],
   "source": [
    "# Expample 1\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that difference between Data Independent Epsilon and Data Dependent Epsilon is even bigger. Our Epsilon is small, we do not leak a lot of personal information and student model prediction is even better with noise!\n",
    "\n",
    "This happend because noise actually helps us to generalize teachers predictions better.\n",
    "\n",
    "Lets try to minimize noise even more with epsilon = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.78515625 .. 0.93359375\n",
      "Data Independent Epsilon: \t 35.03292546497023\n",
      "Data Dependent Epsilon: \t 35.03292546497056\n",
      "Student model accuracy 0.875\n",
      "Student model accuracy (big dataset) 0.8358877301216125\n"
     ]
    }
   ],
   "source": [
    "# Expample 2\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, Data Independent Epsilon and Data Dependent Epsilon are very close to each other. This mean that there is too much noise. Also, student module accyracy droped. Finally, Data Dependent Epsilon shows that teachers are not very much agreed with choosen labels, so we could leak more private date if we remove one teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try something in between. Less noise than in Example 2 but more than in Example 1. If, we added more noise, but if teachers still agree with each other, this could be a better epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.78515625 .. 0.94140625\n",
      "Data Independent Epsilon: \t 105.59292546497024\n",
      "Data Dependent Epsilon: \t 24.116864798722204\n",
      "Student model accuracy 0.83203125\n",
      "Student model accuracy (big dataset) 0.8773885369300842\n"
     ]
    }
   ],
   "source": [
    "# Expample 3\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.80859375 .. 0.95703125\n",
      "Data Independent Epsilon: \t 311.51292546497024\n",
      "Data Dependent Epsilon: \t 6.175096336495198\n",
      "Student model accuracy 0.88671875\n",
      "Student model accuracy (big dataset) 0.887042224407196\n"
     ]
    }
   ],
   "source": [
    "# Expample 4\n",
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like epsilon=0.25 has lowest possible Data Dependent Epsilon estimation. This is not completely true. Because of random nature of noise, Data Dependent Epsilon could increase sighnificantelly if we re-run this cell. So, there is not enough noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is not always possible to increase amount of teacher(thought it might be useful) or force each of them to increase their dataset. \n",
    "Lets have a look on example, where our teacher had very limited datasets. \n",
    "How much noise should we add? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.4739583134651184 .. 0.8211805820465088\n",
      "Data Independent Epsilon: \t 27.51292546497023\n",
      "Data Dependent Epsilon: \t 14.549559989439373\n",
      "Student model accuracy 0.6692708730697632\n",
      "Student model accuracy (big dataset) 0.6566480994224548\n"
     ]
    }
   ],
   "source": [
    "train_subset_size = 100\n",
    "test_subset_size = 100\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=False, eps=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models final accuracies range 0.5017361044883728 .. 0.7899305820465088\n",
      "Data Independent Epsilon: \t 27.51292546497023\n",
      "Data Dependent Epsilon: \t 10.880514411027667\n",
      "Student model accuracy 0.6788194179534912\n",
      "Student model accuracy (big dataset) 0.7127786874771118\n"
     ]
    }
   ],
   "source": [
    "train_subset_size = 100\n",
    "test_subset_size = 100\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "In this work was [PySyft](https://github.com/OpenMined/PySyft) was used to perform analisys. This library still in its early days. It is too early to use it to protect customer data. But in couple of months this statement should be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
