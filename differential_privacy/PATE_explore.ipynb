{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential privacy section project\n",
    "\n",
    "This work is part of [Udacity](http://udacity.com) 'Secure and Private AI scholarship challenge nanodegree Program'. \n",
    "\n",
    "I study there how to protect customer personal data and still be able to train good ML models.\n",
    "Here is my final project in section \"Differential privacy.\"\n",
    "In this project, I'm going to train a Differentially private model using PATE method on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PATE method?\n",
    "Private Aggregation of Teacher Ensembles or PATE.\n",
    "In this approach, the model is not trained on sensitive data directly. \n",
    "Instead, multiple models trained on different subsets of users. These models called **teachers**.\n",
    "Then, the **student** model trained on the noisy output of **teachers** models.\n",
    "\n",
    "As result **student** model doesn't have direct access to data available to **teachers**, but still can make accurate predictions if **teachers** models generalize data very well.\n",
    "\n",
    "### Points of complexity\n",
    "- It is not always possible to identify people in datasets. \n",
    "       One person could write multiple handwritten digits in one set.\n",
    "- Naural models rarely have the same state, even if it was trained on the same dataset.\n",
    "\n",
    "### Task description\n",
    "\n",
    "Let's imagine that we try to predict some labels, but our dataset unlabeled. However, we know that multiple different organizations have labeled data, but they are not going to share raw datasets with us. So we ask these organizations to train models on their side and provide us predictions for our dataset. Then we could label dataset available to us and train our model. Finally, we have to make sure that predictions that were provided for us are protected and there is no privacy leakage.\n",
    "\n",
    "For this task, MNIST dataset should be separated into **teacher**(private) and **student**(public) parts. Each **teacher** dataset will contains both data(images) and targets(labels). **Student** dataset only contains 'data'. After that, ~40 **teachers** models trained on their datasets. Then, each trained model predict targets using public data. Each sample should have ~40 estimations, and the most popular answer selected as a label. The label should not depend on a prediction from a particular teacher, so each label should have some amount of noise. Finally, **student** model trained on a labeled dataset. True labels are removed from **student** datasets artificially in our case. As a result, we could make a decent estimation of prediction quality.\n",
    "\n",
    "The goal is to answer the following questions:\n",
    "\n",
    "- Id it keep *teachers* data private?\n",
    "\n",
    "- Is it possible to train **student** without a massive loss in prediction quality?\n",
    "\n",
    "- What is a necessary amount of noise to add to keep data private?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare datasets for **teachers** and **student** \n",
    "\n",
    "**PyTorch** is the main framework to build and train models in this project. \n",
    "That means that input dataset should be tensors.\n",
    "So, let's download MNIST dataset and make the necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                              ])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = data_utils.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = data_utils.DataLoader(mnist_testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist train size 60000\n",
      "Mnist test size 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mnist train size {len(mnist_trainset)}\")\n",
    "print(f\"Mnist test size {len(mnist_testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use **PyTorch** build-in function `random_split` to split original dataset. `teachers_and_student_datasets` is a convenience function to divide dataset to equally between student and teachers. It helps us to avoid some complexity at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teachers_and_student_datasets(original_dataset, subset_size, parts):\n",
    "    '''divide original dataset to equal subsets'''\n",
    "    total = len(original_dataset)\n",
    "    lengths = [subset_size] * parts + [total - subset_size * parts]\n",
    "    print(f\"Subsets lengths {lengths}\")\n",
    "    datasets = data_utils.dataset.random_split(original_dataset, lengths)\n",
    "    return (datasets[:parts - 1], datasets[parts-1])\n",
    "    \n",
    "def to_dataloader(dataset):\n",
    "    return data_utils.DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train model each **teacher** and **student** should have train and test dataloaders. Also, **teacher** name is useful to track training progress. So, it is not a bad idea to group this data to one named tuple and store together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "DataInfo = namedtuple('DataInfo', 'name train test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was mentioned earlier, **student** dataset only includes images, not labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets):\n",
    "    teachers_train_dl = [to_dataloader(d) for d in teachers_trainsets]\n",
    "    teachers_test_dl = [to_dataloader(d) for d in teachers_testsets] \n",
    "\n",
    "    teachers_data =[DataInfo(a[0], a[1], a[2]) for a in zip(teachers_names, teachers_train_dl, teachers_test_dl)]\n",
    "    return teachers_data\n",
    "\n",
    "def init_students_data(student_name, student_trainset, student_testset):\n",
    "    # train set doesn't have lables\n",
    "    student_train_dl = to_dataloader([img for img, lbl in student_trainset])\n",
    "    student_test_dl = to_dataloader(student_testset)\n",
    "\n",
    "    student_data = DataInfo(student_name, student_train_dl, student_test_dl)\n",
    "    return student_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our infrastructure on toy dataset first. In toy datasets, there are only 6 **teachers** with a set of 100 images each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers_names = ['El', 'Max', 'Dustin', 'Will', 'Lukas', 'Mike']\n",
    "student_name = \"Demogorgon\"\n",
    "subset_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 59300]\n",
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 9300]\n"
     ]
    }
   ],
   "source": [
    "N = len(teachers_names)\n",
    "teachers_trainsets, student_trainset = teachers_and_student_datasets(mnist_trainset, subset_size, N + 1)\n",
    "teachers_testsets, student_testset = teachers_and_student_datasets(mnist_testset, subset_size, N + 1)\n",
    "\n",
    "teachers_data = init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets)\n",
    "student_data = init_students_data(student_name, student_trainset, student_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataInfo(name='Demogorgon', train=<torch.utils.data.dataloader.DataLoader object at 0x116adb668>, test=<torch.utils.data.dataloader.DataLoader object at 0x123c02198>)\n",
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{student_data}\")\n",
    "print(f\"{next(iter(student_data.train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "DataInfo(name='Dustin', train=<torch.utils.data.dataloader.DataLoader object at 0x12715e780>, test=<torch.utils.data.dataloader.DataLoader object at 0x12715eb70>)\n",
      "[tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]), tensor([5, 6, 0, 2, 5, 4, 6, 9, 2, 2, 1, 3, 3, 7, 3, 1, 9, 4, 6, 5, 4, 9, 6, 7,\n",
      "        0, 9, 2, 5, 1, 9, 6, 6, 4, 4, 7, 7, 3, 8, 6, 2, 0, 8, 8, 5, 8, 1, 2, 2,\n",
      "        1, 5, 1, 9, 6, 4, 1, 5, 0, 0, 8, 2, 5, 1, 9, 4])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(teachers_data)}\")\n",
    "print(f\"{teachers_data[2]}\")\n",
    "print(f\"{next(iter(teachers_data[2].train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Select a model\n",
    "Building a perfect predictor is not the primary goal in this project, and the dataset is very simple. So, an unsophisticated, fully connected neural network should be a good fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(true_labels, pred_labels):\n",
    "    '''helps to compare two tensors and return how similar they are in range 0...1'''\n",
    "    equals = pred_labels == true_labels.view(*pred_labels.shape)\n",
    "    return torch.mean(equals.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleModel object is very general, so neither PATE approach or MNIST dataset is mentioned in an object definition. This generalization is an advantage so that it could be easily replaced and changed or served for a different purpose. \n",
    "In this project, **student** and **teachers**  use the same type of model, but this is not required. In real-world, a trained model should be eighter shared as a file or, even better, has a dedicated prediction service with proper logs and authorization. However, I'll skip this part for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.utils.data as ds\n",
    "import functools\n",
    "\n",
    "class SimpleModel():\n",
    "    '''\n",
    "    This is a simple 3-layers fully connected neural network for supervised learning\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name is used to identify model and to track training progress\n",
    "    trainloader : torch.utils.data.dataloader.DataLoader \n",
    "        Dataloader with train dataset\n",
    "    testloader : torch.utils.data.dataloader.DataLoader\n",
    "        Dataloader with test dataset\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str \n",
    "        This is where we store model name\n",
    "    model :\n",
    "        This is where we store model\n",
    "    train_epochs : int\n",
    "        Amount of epochs used to train model (the default is 5)\n",
    "    accuracy : float\n",
    "        Store model's accuracy from the last validation round\n",
    "    is_trained : bool\n",
    "        Flag that shows if model training already finished\n",
    "    '''\n",
    "    \n",
    "    INPUT_SIZE = 784\n",
    "    OUTPUT_SIZE = 10\n",
    "    \n",
    "    def __init__(self, name, trainloader, testloader):\n",
    "        super().__init__()\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.name = name\n",
    "        self.train_epochs = 5\n",
    "        self._accuracy = 0\n",
    "        self._model = None\n",
    "        self._trained = False\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        if self._model is None:\n",
    "            self._model = nn.Sequential(nn.Linear(self.INPUT_SIZE, 256),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(256, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(64, self.OUTPUT_SIZE),\n",
    "                                        nn.LogSoftmax(dim=1))\n",
    "        return self._model\n",
    "                                        \n",
    "    @property\n",
    "    def is_trained(self):\n",
    "        return self._trained\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return self._accuracy\n",
    "    \n",
    "    def predict_labels(self, images):\n",
    "        self.model.eval()\n",
    "        images = self.__flatten_input__(images)\n",
    "        pred = torch.exp(self.model(images))\n",
    "        # get one with highest probablity\n",
    "        top_prob, top_class = pred.topk(1, dim=1)\n",
    "        self.model.train()\n",
    "        return top_class\n",
    "\n",
    "    def train_model(self, verbose=True):\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.005)\n",
    "        self.__opt_print__(f\"{self.name} starts training\", verbose)\n",
    "        for e in range(self.train_epochs):\n",
    "            for images, labels in self.trainloader:\n",
    "                images = self.__flatten_input__(images)\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                accuracy = 0\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in self.testloader:\n",
    "                        pred_labels = self.predict_labels(images)\n",
    "                        accuracy += prediction_accuracy(labels, pred_labels)\n",
    "                self.model.train()\n",
    "                self._accuracy = accuracy/len(self.testloader)\n",
    "                self.__opt_print__(f\"improved accuracy: {self._accuracy}\", verbose)\n",
    "        self._trained = True\n",
    "        self.__opt_print__(f\"{self.name} finished training\", verbose)\n",
    "        return self._model\n",
    "    \n",
    "    def __flatten_input__(self, images):\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        assert(images.shape[1] == self.INPUT_SIZE)\n",
    "        return images\n",
    "    \n",
    "    def __opt_print__(self, message, verbose=True):\n",
    "        if verbose:\n",
    "            print(message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real live **teachers** would be trained on separate machines in parallel. However, because we just fake private datasets **teachers** are trained sequentially and stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El starts training\n",
      "improved accuracy: 0.2560763955116272\n",
      "improved accuracy: 0.4010416865348816\n",
      "improved accuracy: 0.5494791269302368\n",
      "improved accuracy: 0.5729166269302368\n",
      "improved accuracy: 0.6059027910232544\n",
      "El finished training\n",
      "Max starts training\n",
      "improved accuracy: 0.1892361044883728\n",
      "improved accuracy: 0.3758680522441864\n",
      "improved accuracy: 0.3715277910232544\n",
      "improved accuracy: 0.3524305522441864\n",
      "improved accuracy: 0.5251736044883728\n",
      "Max finished training\n",
      "Dustin starts training\n",
      "improved accuracy: 0.2734375\n",
      "improved accuracy: 0.5416666269302368\n",
      "improved accuracy: 0.5989583730697632\n",
      "improved accuracy: 0.6536458730697632\n",
      "improved accuracy: 0.6831597089767456\n",
      "Dustin finished training\n",
      "Will starts training\n",
      "improved accuracy: 0.1414930522441864\n",
      "improved accuracy: 0.3602430522441864\n",
      "improved accuracy: 0.4956597089767456\n",
      "improved accuracy: 0.5017361044883728\n",
      "improved accuracy: 0.6215277910232544\n",
      "Will finished training\n",
      "Lukas starts training\n",
      "improved accuracy: 0.3958333134651184\n",
      "improved accuracy: 0.5321180820465088\n",
      "improved accuracy: 0.6119791269302368\n",
      "improved accuracy: 0.6675347089767456\n",
      "improved accuracy: 0.7065972089767456\n",
      "Lukas finished training\n",
      "Mike starts training\n",
      "improved accuracy: 0.2934027910232544\n",
      "improved accuracy: 0.5824652910232544\n",
      "improved accuracy: 0.5885416269302368\n",
      "improved accuracy: 0.6432291269302368\n",
      "improved accuracy: 0.6571180820465088\n",
      "Mike finished training\n"
     ]
    }
   ],
   "source": [
    "teachers = []\n",
    "for (name, trainloader, testloader) in teachers_data:\n",
    "    t = SimpleModel(name, trainloader, testloader)\n",
    "    t.train_model()\n",
    "    teachers.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks fine. Datasets are small and are not very good predictors as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Public dataset labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to get a prediction from each teacher for every sample. This is function `predict_labels_per_teacher` is for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_per_teacher(images, teachers):\n",
    "    batch_size = images.shape[0]\n",
    "    preds = np.empty([0, batch_size], dtype=int)\n",
    "    for t in teachers:\n",
    "        p = t.predict_labels(images).view(1, batch_size).numpy()\n",
    "        preds = np.append(preds, p, axis=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, for each sample, we should identify the most popular prediction using `bincount` and `argmax` functions and add noise to a prediction.\n",
    "\n",
    "In function `select_best_labels` we use differential privacy to protect customers data.\n",
    "If every teacher generalizes their models very well, then all teachers should agree on every label. In this case, we don't need to add noise to protect personal data: any teacher could be removed, but predicted labels stay the same. \n",
    "On the other hand, if different teacher predicts different labels for the same sample than more noise required to protect personal data. \n",
    "\n",
    "**Why should we add noise?** Imagine that **teachers** are hospitals and our model try to predict some sensitive information, for example, some disease. Also, let's imagine that all hospitals don't agree much in their predictions. In this case, if at least one hospital is removed from the process, predicted labels would change. In this case, we could deduct removed hospital bias and make an estimation of what diseases its customers most probably had. If the disease is rare than given information could be even linked to a real person. (although it requires access to some other private datasets or information).\n",
    "\n",
    "**How much noise to add?** \n",
    "In this project, we generated noise using Laplacian distribution with a peak around 0 and beta defined as: \n",
    "\n",
    "`beta = sensitivity / epsilon`. \n",
    "\n",
    "Sensitivity shows how much information we are leaking with one model prediction request. Epsilon identifies the maximum acceptable privacy leakage. \n",
    "\n",
    "Sensitivity value: \n",
    "In the worst-case, each teacher associated with exactly one person. So, at maximum, one person can change a label prediction by 1. In this case, the sensitivity of model prediction per each sample per teacher = 1. \n",
    "Epsilon value:\n",
    "In general, we want to minimize epsilon. However, if it is too small, the beta should be huge, that means an enormous amount of noise in predictions. Noisy predictions lead to the poor performance of **student** model. So, we start with a value 0.25 and then try to minimize it, without a significant loss in prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_labels(preds, use_noise=True, eps=0.25, num_labels=10):\n",
    "    num_samples = preds.shape[1]\n",
    "    labels = np.empty([0], dtype=int)\n",
    "    for i in range(num_samples):\n",
    "        counts = np.bincount(preds[:, i], minlength=num_labels)\n",
    "        if use_noise:\n",
    "            sensitivity = 1\n",
    "            beta = sensitivity / eps\n",
    "            noise = np.random.laplace(0, beta, len(counts)).astype(int)\n",
    "            counts += noise\n",
    "        labels = np.append(labels, np.argmax(counts))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function ```perform_analysis```  helps to identify optimal epsilon value.\n",
    "Framework **PySyft** contains handy function `pate.perform_analysis` for this kind of tasks. It helps to identify how much teachers agree with each other to identify optimal epsilon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 22:16:30.153984 4680693184 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/miniconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0724 22:16:30.172158 4680693184 deprecation_wrapper.py:119] From /miniconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "def perform_analysis(imageset, teachers, use_noise=True, eps=0.25, num_labels=10):\n",
    "    all_preds = np.empty([len(teachers), 0], dtype=int)\n",
    "    all_labels = np.empty([0], dtype=int)\n",
    "    for images in imageset:\n",
    "        preds = predict_labels_per_teacher(images, teachers)\n",
    "        labels = select_best_labels(preds, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "        all_preds = np.append(all_preds, preds, axis=1)\n",
    "        all_labels = np.append(all_labels, labels)\n",
    "    return pate.perform_analysis(teacher_preds=all_preds, indices=all_labels, noise_eps=eps, delta=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how new function works on a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 36.51292546497023\n",
      "Data Dependent Epsilon: 36.51292546497023\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = perform_analysis(student_data.train, teachers, use_noise=True, eps=0.25, num_labels=10)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Independent and Dependent Epsilon are very close. There could be at least two reasons why this happened. Either our teachers predict labels almost randomly, or there is too much noise added. In this case, it is both. Indeed Laplace distribution with `beta = 1 / 0.25` generates noise that alters predictions completely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's create a function ```label_dataset ``` that uses all described functions to create a labeled dataset for **student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dataset(imageset, teachers, use_noise=True, eps=0.25, num_labels=10):\n",
    "    ''' Create labeled dataset from images and teachers that helps to predict it '''\n",
    "    datasets = []\n",
    "    for images in imageset:\n",
    "        preds = predict_labels_per_teacher(images, teachers)\n",
    "        labels = select_best_labels(preds, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "        datasets.append(data_utils.TensorDataset(images, torch.tensor(labels)))\n",
    "    return data_utils.ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Check if it is needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def model_accuracy_range(model, dataloader, trials=15):\n",
    "    '''Run predictions multiple times to generate more precise estimation'''\n",
    "    min_accuracy = sys.float_info.max - 1; max_accuracy = 0\n",
    "    for i in range(trials):\n",
    "        accuracy = 0 \n",
    "        for images, true_labels in dataloader:\n",
    "            labels = model.predict_labels(images)\n",
    "            accuracy += prediction_accuracy(true_labels, labels)\n",
    "        accuracy = (accuracy / len(mnist_testloader)).numpy()\n",
    "        min_accuracy = min(min_accuracy, accuracy)\n",
    "        max_accuracy = max(max_accuracy, accuracy)\n",
    "    return (min_accuracy, max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Monitor noise impact on prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all three parts and make 'main' function that runs it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.25, num_labels=SimpleModel.OUTPUT_SIZE):\n",
    "    # generate data from MNIST\n",
    "    N = len(teachers_names)\n",
    "    teachers_trainsets, student_trainset = teachers_and_student_datasets(mnist_trainset, train_subset_size, N + 1)\n",
    "    teachers_testsets, student_testset = teachers_and_student_datasets(mnist_testset, test_subset_size, N + 1)\n",
    "\n",
    "    teachers_data = init_teachers_data(teachers_names, teachers_trainsets, teachers_testsets)\n",
    "    student_data = init_students_data(student_name, student_trainset, student_testset)\n",
    "    \n",
    "    # train teachers models\n",
    "    teachers = []\n",
    "    for (name, trainloader, testloader) in teachers_data:\n",
    "        t = SimpleModel(name, trainloader, testloader)\n",
    "        t.train_model(verbose=False)\n",
    "        print(f\"Model {name} final accuracy {t.accuracy}\")\n",
    "        teachers.append(t)\n",
    "    \n",
    "    # evaluate Epsilon\n",
    "    data_dep_eps, data_ind_eps = perform_analysis(student_data.train, teachers, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "    print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "    print(\"Data Dependent Epsilon:\", data_dep_eps)   \n",
    "\n",
    "    # create labeled dataset\n",
    "    result_dataset = labeled_dataset(student_data.train, teachers, use_noise=use_noise, eps=eps, num_labels=num_labels)\n",
    "    result_dataloader = data_utils.DataLoader(result_dataset, batch_size=64, shuffle=True)\n",
    "    result_model = SimpleModel(student_data.name, result_dataloader, student_data.test)\n",
    "    result_model.train_model()\n",
    "    print(f\"New model accuracy {result_model.accuracy}\")\n",
    "    \n",
    "    # get new model accuracy range\n",
    "    min_accuracy, max_accuracy = model_accuracy_range(result_model, mnist_testloader, trials=15)\n",
    "    print(f\"New model accuracy (big dataset) range {min_accuracy}..{max_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 49\n",
    "teachers_names = [f'office_{i}' for i in range(49)]\n",
    "student_name = \"Reseach office\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, lets try to add some noise to a models that are not very good predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 55000]\n",
      "Subsets lengths [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 5000]\n",
      "Model office_0 final accuracy 0.6519097089767456\n",
      "Model office_1 final accuracy 0.59375\n",
      "Model office_2 final accuracy 0.7404513955116272\n",
      "Model office_3 final accuracy 0.7161458730697632\n",
      "Model office_4 final accuracy 0.6692708730697632\n",
      "Model office_5 final accuracy 0.7083333730697632\n",
      "Model office_6 final accuracy 0.6970486044883728\n",
      "Model office_7 final accuracy 0.6848958730697632\n",
      "Model office_8 final accuracy 0.6597222089767456\n",
      "Model office_9 final accuracy 0.7465277910232544\n",
      "Model office_10 final accuracy 0.7083333730697632\n",
      "Model office_11 final accuracy 0.6336805820465088\n",
      "Model office_12 final accuracy 0.7855902910232544\n",
      "Model office_13 final accuracy 0.71875\n",
      "Model office_14 final accuracy 0.6909722089767456\n",
      "Model office_15 final accuracy 0.7317708730697632\n",
      "Model office_16 final accuracy 0.7421875\n",
      "Model office_17 final accuracy 0.6371527910232544\n",
      "Model office_18 final accuracy 0.5763888955116272\n",
      "Model office_19 final accuracy 0.6293402910232544\n",
      "Model office_20 final accuracy 0.6458333730697632\n",
      "Model office_21 final accuracy 0.7109375\n",
      "Model office_22 final accuracy 0.6892361044883728\n",
      "Model office_23 final accuracy 0.59375\n",
      "Model office_24 final accuracy 0.578125\n",
      "Model office_25 final accuracy 0.71875\n",
      "Model office_26 final accuracy 0.6796875\n",
      "Model office_27 final accuracy 0.6440972089767456\n",
      "Model office_28 final accuracy 0.7543402910232544\n",
      "Model office_29 final accuracy 0.6571180820465088\n",
      "Model office_30 final accuracy 0.6614583730697632\n",
      "Model office_31 final accuracy 0.7387152910232544\n",
      "Model office_32 final accuracy 0.6605902910232544\n",
      "Model office_33 final accuracy 0.6649305820465088\n",
      "Model office_34 final accuracy 0.5529513955116272\n",
      "Model office_35 final accuracy 0.7282986044883728\n"
     ]
    }
   ],
   "source": [
    "train_subset_size = 100\n",
    "test_subset_size = 100\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_size = 100\n",
    "test_subset_size = 100\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=False, eps=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=True, eps=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_size = 1200\n",
    "test_subset_size = 200\n",
    "run(teachers_names, student_name, train_subset_size, test_subset_size, use_noise=False, eps=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "In this work was [PySyft](https://github.com/OpenMined/PySyft) was used to perform analisys. This library still in its early days. It is too early to use it to protect customer data. But in couple of months this statement should be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
